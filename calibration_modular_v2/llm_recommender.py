"""
LLM-based Parameter Recommender for Calibration Experiments

This module provides LLM-based parameter suggestions for pipetting calibration.
It integrates with the template-based LLM configuration system and supports
both screening and optimization phases.
"""

import json
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from pathlib import Path

from data_structures import PipettingParameters, VolumeCalibrationResult
from config_manager import ExperimentConfig
from llm_config_generator import LLMConfigGenerator


class LLMRecommender:
    """LLM-based parameter recommender using template-generated configurations."""
    
    def __init__(self, config: ExperimentConfig, llm_config_path: str, phase: str = "screening"):
        """
        Initialize LLM recommender.
        
        Args:
            config: Experiment configuration
            llm_config_path: Path to LLM configuration file (generated by LLMConfigGenerator)
            phase: "screening" or "optimization" phase
        """
        self.config = config
        self.phase = phase
        self.logger = logging.getLogger(__name__)
        
        # Load LLM configuration
        self.llm_config = self._load_llm_config(llm_config_path)
        
        # Extract parameter information
        self._setup_parameters()
        
        # Initialize conversation history
        self.conversation_history = []
        
    def _load_llm_config(self, config_path: str) -> Dict[str, Any]:
        """Load LLM configuration from file."""
        try:
            with open(config_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            raise ValueError(f"Failed to load LLM config from {config_path}: {e}")
    
    def _setup_parameters(self):
        """Setup parameter bounds and constraints."""
        self.optimize_params = []
        self.fixed_params = {}
        self.param_bounds = {}
        
        for param_name, param_config in self.config.parameters.items():
            if param_config.fixed is not None:
                self.fixed_params[param_name] = param_config.fixed
            else:
                self.optimize_params.append(param_name)
                self.param_bounds[param_name] = param_config.bounds
    
    def suggest_parameters(self, n_suggestions: int = 1, 
                          previous_results: Optional[List[VolumeCalibrationResult]] = None) -> List[PipettingParameters]:
        """
        Generate LLM-based parameter suggestions.
        
        Args:
            n_suggestions: Number of parameter sets to suggest
            previous_results: Previous calibration results for context
            
        Returns:
            List of suggested parameter sets
        """
        # Build context from previous results
        context = self._build_context(previous_results)
        
        # Generate LLM prompt
        prompt = self._generate_prompt(n_suggestions, context)
        
        # For now, simulate LLM response with intelligent random suggestions
        # In production, this would call actual LLM API
        suggestions = self._simulate_llm_response(n_suggestions, context)
        
        # Log suggestions
        self.logger.info(f"LLM recommender generated {len(suggestions)} suggestions for {self.phase} phase")
        
        return suggestions
    
    def _build_context(self, previous_results: Optional[List[VolumeCalibrationResult]]) -> Dict[str, Any]:
        """Build context dictionary from previous results."""
        if not previous_results:
            return {
                "phase": self.phase,
                "has_previous_data": False,
                "liquid": self.config.experiment.liquid,
                "target_volume": None
            }
        
        # Analyze previous results
        best_trials = []
        for result in previous_results:
            best_trials.extend(result.best_trials)
        
        if not best_trials:
            return {
                "phase": self.phase,
                "has_previous_data": False,
                "liquid": self.config.experiment.liquid,
                "target_volume": previous_results[0].target_volume_ml if previous_results else None
            }
        
        # Find overall best trial
        best_trial = min(best_trials, key=lambda t: t.score)
        
        # Calculate summary statistics
        all_scores = [t.score for t in best_trials]
        all_durations = [t.duration_s for t in best_trials]
        
        results_summary = {
            "best_score": min(all_scores),
            "avg_score": np.mean(all_scores),
            "avg_duration": np.mean(all_durations),
            "num_trials": len(best_trials),
            "num_volumes": len(previous_results)
        }
        
        return {
            "phase": self.phase,
            "has_previous_data": True,
            "liquid": self.config.experiment.liquid,
            "target_volume": previous_results[-1].target_volume_ml,  # Most recent volume
            "results_summary": results_summary,
            "best_parameters": {
                "aspirate_speed": best_trial.parameters.aspirate_speed,
                "dispense_speed": best_trial.parameters.dispense_speed,
                "aspirate_wait_time": best_trial.parameters.aspirate_wait_time,
                "dispense_wait_time": best_trial.parameters.dispense_wait_time,
                "retract_speed": best_trial.parameters.retract_speed,
                "blowout_vol": best_trial.parameters.blowout_vol,
                "post_asp_air_vol": best_trial.parameters.post_asp_air_vol,
                "overaspirate_vol": best_trial.parameters.overaspirate_vol
            }
        }
    
    def _generate_prompt(self, n_suggestions: int, context: Dict[str, Any]) -> str:
        """Generate LLM prompt using template system."""
        system_message = self.llm_config["system_message"]
        
        # Build user prompt based on context
        if context["has_previous_data"]:
            user_prompt = f"""
Please suggest {n_suggestions} parameter set(s) for {context['liquid']} calibration.

Previous results summary:
- Best score: {context['results_summary']['best_score']:.3f}
- Average score: {context['results_summary']['avg_score']:.3f}
- Average duration: {context['results_summary']['avg_duration']:.1f}s
- Number of trials: {context['results_summary']['num_trials']}
- Volumes tested: {context['results_summary']['num_volumes']}

Current target volume: {context['target_volume']:.3f} mL

Best performing parameters so far:
{json.dumps(context['best_parameters'], indent=2)}

Please suggest improvements or variations that might achieve better performance.
"""
        else:
            user_prompt = f"""
Please suggest {n_suggestions} parameter set(s) for {context['liquid']} calibration.
This is the {context['phase']} phase with no previous experimental data.
Focus on exploring the parameter space effectively.
"""
        
        return f"System: {system_message}\n\nUser: {user_prompt}"
    
    def _simulate_llm_response(self, n_suggestions: int, context: Dict[str, Any]) -> List[PipettingParameters]:
        """
        Simulate intelligent LLM response.
        In production, replace with actual LLM API call.
        """
        suggestions = []
        
        for i in range(n_suggestions):
            param_values = {}
            
            # Generate intelligent suggestions based on context
            if context["has_previous_data"] and context["phase"] == "optimization":
                # Optimization phase: refine around best parameters
                best_params = context["best_parameters"]
                for param_name in self.optimize_params:
                    bounds = self.param_bounds[param_name]
                    if param_name in best_params:
                        # Add noise around best value
                        best_val = best_params[param_name]
                        noise_factor = 0.1  # 10% variation
                        range_size = bounds[1] - bounds[0]
                        noise = np.random.normal(0, noise_factor * range_size)
                        suggested_val = np.clip(best_val + noise, bounds[0], bounds[1])
                    else:
                        # Random if no previous data for this parameter
                        suggested_val = np.random.uniform(bounds[0], bounds[1])
                    param_values[param_name] = suggested_val
            else:
                # Screening phase: intelligent exploration
                for param_name in self.optimize_params:
                    bounds = self.param_bounds[param_name]
                    
                    # Use domain knowledge for better initial suggestions
                    if param_name in ["aspirate_speed", "dispense_speed"]:
                        # Moderate speeds tend to work well
                        mid_point = (bounds[0] + bounds[1]) / 2
                        param_values[param_name] = np.random.normal(mid_point, (bounds[1] - bounds[0]) / 6)
                        param_values[param_name] = np.clip(param_values[param_name], bounds[0], bounds[1])
                    elif param_name in ["aspirate_wait_time", "dispense_wait_time"]:
                        # Longer wait times often help with accuracy
                        param_values[param_name] = np.random.uniform(bounds[0] + 0.3 * (bounds[1] - bounds[0]), bounds[1])
                    else:
                        # Other parameters: uniform random
                        param_values[param_name] = np.random.uniform(bounds[0], bounds[1])
            
            # Add fixed parameters
            param_values.update(self.fixed_params)
            
            # Fill any missing parameters with defaults
            for param_name, param_config in self.config._config['parameters'].items():
                if param_name not in param_values:
                    param_values[param_name] = param_config['default']
            
            suggestions.append(PipettingParameters(**param_values))
        
        return suggestions
    
    def update_with_results(self, results: List[VolumeCalibrationResult]):
        """Update recommender with new experimental results."""
        # In production LLM system, this would update conversation history
        # For simulation, we just log the update
        self.logger.info(f"Updated LLM recommender with {len(results)} new results")
        
        # Store results for future context
        if not hasattr(self, 'all_results'):
            self.all_results = []
        self.all_results.extend(results)


def create_llm_recommender(config: ExperimentConfig, phase: str = "screening") -> Optional[LLMRecommender]:
    """
    Create LLM recommender if enabled in configuration.
    
    Args:
        config: Experiment configuration
        phase: "screening" or "optimization"
        
    Returns:
        LLMRecommender instance or None if disabled
    """
    if phase == "screening":
        if not config.use_llm_for_screening():
            return None
        config_path = config.get_screening_llm_config_path()
    else:  # optimization
        if not config.is_llm_optimization_enabled():
            return None  
        config_path = config.get_llm_config_path()
    
    if not config_path:
        # Generate LLM config on-the-fly
        generator = LLMConfigGenerator(config)
        config_path = f"generated_llm_config_{phase}.json"
        generator.generate_config(config_path, phase)
    
    return LLMRecommender(config, config_path, phase)